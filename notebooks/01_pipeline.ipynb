{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NYC Noise Complaints and Liquor License Analysis Pipeline\n",
    "\n",
    "This notebook implements a complete data processing and machine learning pipeline for analyzing the relationship between noise complaints and liquor-licensed establishments in NYC.\n",
    "\n",
    "## Project Overview\n",
    "- **Goal**: Analyze noise complaints associated with areas that have liquor licenses\n",
    "- **Data Sources**: \n",
    "  - NYC 311 Noise Complaints (311_noise.csv)\n",
    "  - NYC State Liquor Authority (SLA) Active Licenses (sla_active.csv)\n",
    "\n",
    "## Pipeline Stages\n",
    "1. Data Loading and Exploration\n",
    "2. Data Preprocessing\n",
    "3. Spatial Analysis and Feature Engineering\n",
    "4. Model Training and Evaluation\n",
    "5. Results and Insights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.metrics import classification_report, confusion_matrix, mean_squared_error, r2_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Set visualization style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "# Random seed for reproducibility\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "print('Environment setup complete!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading\n",
    "\n",
    "Load the SLA liquor license data and prepare for 311 noise complaints data integration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data paths\n",
    "DATA_DIR = Path('../data')\n",
    "RAW_DIR = DATA_DIR / 'raw'\n",
    "INTERIM_DIR = DATA_DIR / 'interim'\n",
    "PROCESSED_DIR = DATA_DIR / 'processed'\n",
    "\n",
    "# Create directories if they don't exist\n",
    "INTERIM_DIR.mkdir(parents=True, exist_ok=True)\n",
    "PROCESSED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Load SLA liquor license data\n",
    "sla_path = RAW_DIR / 'sla_active.csv'\n",
    "noise_path = RAW_DIR / '311_noise.csv'\n",
    "\n",
    "print(f'Loading liquor license data from: {sla_path}')\n",
    "df_sla = pd.read_csv(sla_path)\n",
    "print(f'Loaded {len(df_sla)} liquor license records')\n",
    "print(f'\\nSLA Data Shape: {df_sla.shape}')\n",
    "print(f'\\nSLA Data Columns: {df_sla.columns.tolist()}')\n",
    "\n",
    "# Check for noise complaints data\n",
    "if noise_path.exists():\n",
    "    print(f'\\nLoading noise complaints data from: {noise_path}')\n",
    "    df_noise = pd.read_csv(noise_path)\n",
    "    print(f'Loaded {len(df_noise)} noise complaint records')\n",
    "    print(f'Noise Data Shape: {df_noise.shape}')\n",
    "else:\n",
    "    print(f'\\nNote: 311 noise complaints data not found at {noise_path}')\n",
    "    print('To complete the analysis, please download NYC 311 data and place it in data/raw/')\n",
    "    print('The pipeline will continue with liquor license data only.')\n",
    "    df_noise = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Exploration and Understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display basic information about SLA data\n",
    "print('=== SLA Liquor License Data Overview ===')\n",
    "print(f'\\nFirst few records:')\n",
    "display(df_sla.head())\n",
    "\n",
    "print(f'\\nData Types:')\n",
    "print(df_sla.dtypes)\n",
    "\n",
    "print(f'\\nMissing Values:')\n",
    "missing = df_sla.isnull().sum()\n",
    "print(missing[missing > 0])\n",
    "\n",
    "print(f'\\nBasic Statistics:')\n",
    "display(df_sla.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze license types and descriptions\n",
    "print('=== License Type Distribution ===')\n",
    "print(df_sla['Description'].value_counts())\n",
    "\n",
    "# Plot distribution of license types\n",
    "plt.figure(figsize=(12, 6))\n",
    "df_sla['Description'].value_counts().plot(kind='bar')\n",
    "plt.title('Distribution of Liquor License Types')\n",
    "plt.xlabel('License Type')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Preprocessing\n",
    "\n",
    "Clean and prepare the data for analysis and modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy for processing\n",
    "df_processed = df_sla.copy()\n",
    "\n",
    "# Extract coordinates from Georeference column\n",
    "def extract_coordinates(georeference):\n",
    "    \"\"\"Extract latitude and longitude from POINT string\"\"\"\n",
    "    try:\n",
    "        coords = georeference.replace('POINT (', '').replace(')', '')\n",
    "        lon, lat = coords.split()\n",
    "        return float(lat), float(lon)\n",
    "    except:\n",
    "        return None, None\n",
    "\n",
    "df_processed[['Latitude', 'Longitude']] = df_processed['Georeference'].apply(\n",
    "    lambda x: pd.Series(extract_coordinates(x))\n",
    ")\n",
    "\n",
    "print(f'Coordinates extracted. Records with valid coordinates: {df_processed[\"Latitude\"].notna().sum()}')\n",
    "\n",
    "# Convert date columns to datetime\n",
    "date_columns = ['Original Issue Date', 'Last Issue Date', 'Effective Date', 'Expiration Date']\n",
    "for col in date_columns:\n",
    "    df_processed[col] = pd.to_datetime(df_processed[col], errors='coerce')\n",
    "\n",
    "# Calculate license age and days until expiration\n",
    "df_processed['License_Age_Days'] = (pd.Timestamp.now() - df_processed['Original Issue Date']).dt.days\n",
    "df_processed['Days_Until_Expiration'] = (df_processed['Expiration Date'] - pd.Timestamp.now()).dt.days\n",
    "\n",
    "# Fill missing DBA with business name\n",
    "df_processed['DBA'] = df_processed['DBA'].fillna(df_processed['LegalName'])\n",
    "\n",
    "# Create binary feature for whether license is close to expiration\n",
    "df_processed['Expiring_Soon'] = (df_processed['Days_Until_Expiration'] < 90).astype(int)\n",
    "\n",
    "print('\\nData preprocessing complete!')\n",
    "print(f'Processed data shape: {df_processed.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save interim processed data\n",
    "interim_path = INTERIM_DIR / 'sla_processed.csv'\n",
    "df_processed.to_csv(interim_path, index=False)\n",
    "print(f'Interim data saved to: {interim_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Engineering\n",
    "\n",
    "Create features for machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features for modeling\n",
    "feature_columns = [\n",
    "    'Type', 'Class', 'Description', 'Premises County',\n",
    "    'Latitude', 'Longitude', 'License_Age_Days', \n",
    "    'Days_Until_Expiration', 'Zip Code'\n",
    "]\n",
    "\n",
    "# Create feature dataframe\n",
    "df_features = df_processed[feature_columns].copy()\n",
    "\n",
    "# Drop rows with missing critical values\n",
    "df_features = df_features.dropna(subset=['Latitude', 'Longitude'])\n",
    "\n",
    "print(f'Feature engineering complete!')\n",
    "print(f'Features shape: {df_features.shape}')\n",
    "print(f'\\nFeatures: {df_features.columns.tolist()}')\n",
    "print(f'\\nFeature data types:\\n{df_features.dtypes}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Noise Complaint Integration (When Available)\n",
    "\n",
    "This section will integrate noise complaint data when available. For demonstration, we'll create synthetic target variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For demonstration purposes, create synthetic target variables\n",
    "# In a real scenario, these would come from spatial join with noise complaint data\n",
    "\n",
    "# Target 1: Number of noise complaints within radius (regression)\n",
    "# Simulate based on license type - restaurants typically generate more noise\n",
    "np.random.seed(RANDOM_STATE)\n",
    "base_complaints = np.random.poisson(lam=5, size=len(df_features))\n",
    "restaurant_multiplier = (df_features['Description'] == 'Restaurant').astype(int) * 3\n",
    "df_features['Noise_Complaints_Count'] = base_complaints + restaurant_multiplier + np.random.poisson(lam=2, size=len(df_features))\n",
    "\n",
    "# Target 2: High noise area classification (binary)\n",
    "df_features['High_Noise_Area'] = (df_features['Noise_Complaints_Count'] > df_features['Noise_Complaints_Count'].median()).astype(int)\n",
    "\n",
    "print('Target variables created:')\n",
    "print(f'- Noise_Complaints_Count (regression target): min={df_features[\"Noise_Complaints_Count\"].min()}, max={df_features[\"Noise_Complaints_Count\"].max()}, mean={df_features[\"Noise_Complaints_Count\"].mean():.2f}')\n",
    "print(f'- High_Noise_Area (classification target): {df_features[\"High_Noise_Area\"].value_counts().to_dict()}')\n",
    "\n",
    "# Visualize target distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].hist(df_features['Noise_Complaints_Count'], bins=20, edgecolor='black')\n",
    "axes[0].set_title('Distribution of Noise Complaints Count')\n",
    "axes[0].set_xlabel('Number of Complaints')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "\n",
    "df_features['High_Noise_Area'].value_counts().plot(kind='bar', ax=axes[1])\n",
    "axes[1].set_title('High vs Low Noise Areas')\n",
    "axes[1].set_xlabel('High Noise Area (1=Yes, 0=No)')\n",
    "axes[1].set_ylabel('Count')\n",
    "axes[1].set_xticklabels(['Low', 'High'], rotation=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Machine Learning Pipeline - Classification\n",
    "\n",
    "Build a pipeline to predict high noise areas based on liquor license characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for classification\n",
    "X = df_features.drop(columns=['Noise_Complaints_Count', 'High_Noise_Area'])\n",
    "y_classification = df_features['High_Noise_Area']\n",
    "\n",
    "# Identify numeric and categorical features\n",
    "numeric_features = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "categorical_features = X.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "print(f'Numeric features: {numeric_features}')\n",
    "print(f'Categorical features: {categorical_features}')\n",
    "\n",
    "# Create preprocessing pipelines\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "\n",
    "# Combine preprocessing steps\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "# Create full pipeline with classifier\n",
    "clf_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', RandomForestClassifier(n_estimators=100, random_state=RANDOM_STATE, max_depth=10))\n",
    "])\n",
    "\n",
    "print('\\nClassification pipeline created!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_classification, test_size=0.2, random_state=RANDOM_STATE, stratify=y_classification\n",
    ")\n",
    "\n",
    "print(f'Training set size: {len(X_train)}')\n",
    "print(f'Test set size: {len(X_test)}')\n",
    "print(f'\\nClass distribution in training set:\\n{y_train.value_counts()}')\n",
    "\n",
    "# Train the model\n",
    "print('\\nTraining classification model...')\n",
    "clf_pipeline.fit(X_train, y_train)\n",
    "print('Training complete!')\n",
    "\n",
    "# Make predictions\n",
    "y_pred_train = clf_pipeline.predict(X_train)\n",
    "y_pred_test = clf_pipeline.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "train_score = clf_pipeline.score(X_train, y_train)\n",
    "test_score = clf_pipeline.score(X_test, y_test)\n",
    "\n",
    "print(f'\\n=== Classification Results ===')\n",
    "print(f'Training Accuracy: {train_score:.4f}')\n",
    "print(f'Test Accuracy: {test_score:.4f}')\n",
    "print(f'\\nTest Set Classification Report:')\n",
    "print(classification_report(y_test, y_pred_test, target_names=['Low Noise', 'High Noise']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred_test)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Low Noise', 'High Noise'],\n",
    "            yticklabels=['Low Noise', 'High Noise'])\n",
    "plt.title('Confusion Matrix - High Noise Area Classification')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance analysis\n",
    "try:\n",
    "    # Get feature names after preprocessing\n",
    "    feature_names = (numeric_features + \n",
    "                    clf_pipeline.named_steps['preprocessor']\n",
    "                    .named_transformers_['cat']\n",
    "                    .named_steps['onehot']\n",
    "                    .get_feature_names_out(categorical_features).tolist())\n",
    "    \n",
    "    # Get feature importances\n",
    "    importances = clf_pipeline.named_steps['classifier'].feature_importances_\n",
    "    \n",
    "    # Create dataframe and sort\n",
    "    feature_importance_df = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'importance': importances\n",
    "    }).sort_values('importance', ascending=False).head(15)\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(range(len(feature_importance_df)), feature_importance_df['importance'])\n",
    "    plt.yticks(range(len(feature_importance_df)), feature_importance_df['feature'])\n",
    "    plt.xlabel('Importance')\n",
    "    plt.title('Top 15 Feature Importances for Noise Prediction')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print('\\nTop 10 Most Important Features:')\n",
    "    print(feature_importance_df.head(10).to_string(index=False))\n",
    "except Exception as e:\n",
    "    print(f'Feature importance analysis skipped: {e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Machine Learning Pipeline - Regression\n",
    "\n",
    "Build a pipeline to predict the number of noise complaints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for regression\n",
    "y_regression = df_features['Noise_Complaints_Count']\n",
    "\n",
    "# Create regression pipeline\n",
    "reg_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', RandomForestRegressor(n_estimators=100, random_state=RANDOM_STATE, max_depth=10))\n",
    "])\n",
    "\n",
    "# Split data\n",
    "X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(\n",
    "    X, y_regression, test_size=0.2, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "print('Training regression model...')\n",
    "reg_pipeline.fit(X_train_reg, y_train_reg)\n",
    "print('Training complete!')\n",
    "\n",
    "# Make predictions\n",
    "y_pred_train_reg = reg_pipeline.predict(X_train_reg)\n",
    "y_pred_test_reg = reg_pipeline.predict(X_test_reg)\n",
    "\n",
    "# Evaluate\n",
    "train_r2 = r2_score(y_train_reg, y_pred_train_reg)\n",
    "test_r2 = r2_score(y_test_reg, y_pred_test_reg)\n",
    "train_rmse = np.sqrt(mean_squared_error(y_train_reg, y_pred_train_reg))\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test_reg, y_pred_test_reg))\n",
    "\n",
    "print(f'\\n=== Regression Results ===')\n",
    "print(f'Training R²: {train_r2:.4f}')\n",
    "print(f'Test R²: {test_r2:.4f}')\n",
    "print(f'Training RMSE: {train_rmse:.4f}')\n",
    "print(f'Test RMSE: {test_rmse:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot predictions vs actual\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test_reg, y_pred_test_reg, alpha=0.6, edgecolors='k')\n",
    "plt.plot([y_test_reg.min(), y_test_reg.max()], \n",
    "         [y_test_reg.min(), y_test_reg.max()], \n",
    "         'r--', lw=2, label='Perfect Prediction')\n",
    "plt.xlabel('Actual Noise Complaints')\n",
    "plt.ylabel('Predicted Noise Complaints')\n",
    "plt.title(f'Regression: Predicted vs Actual (R² = {test_r2:.4f})')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Save Models and Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Create models directory\n",
    "MODELS_DIR = Path('../models')\n",
    "MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save models\n",
    "clf_model_path = MODELS_DIR / 'noise_classifier.pkl'\n",
    "reg_model_path = MODELS_DIR / 'noise_regressor.pkl'\n",
    "\n",
    "joblib.dump(clf_pipeline, clf_model_path)\n",
    "joblib.dump(reg_pipeline, reg_model_path)\n",
    "\n",
    "print(f'Classification model saved to: {clf_model_path}')\n",
    "print(f'Regression model saved to: {reg_model_path}')\n",
    "\n",
    "# Save processed features\n",
    "processed_data_path = PROCESSED_DIR / 'features_with_targets.csv'\n",
    "df_features.to_csv(processed_data_path, index=False)\n",
    "print(f'\\nProcessed features saved to: {processed_data_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary and Next Steps\n",
    "\n",
    "### What We've Accomplished:\n",
    "1. ✅ Loaded and explored NYC SLA liquor license data\n",
    "2. ✅ Performed data preprocessing and feature engineering\n",
    "3. ✅ Created synthetic noise complaint targets for demonstration\n",
    "4. ✅ Built and evaluated classification pipeline (High/Low noise areas)\n",
    "5. ✅ Built and evaluated regression pipeline (Noise complaint count)\n",
    "6. ✅ Saved trained models and processed data\n",
    "\n",
    "### Next Steps:\n",
    "1. **Integrate Real 311 Data**: Download NYC 311 noise complaint data and place in `data/raw/311_noise.csv`\n",
    "2. **Spatial Analysis**: Perform spatial joins to match liquor licenses with nearby noise complaints\n",
    "3. **Advanced Features**: \n",
    "   - Time-based features (day of week, time of day for complaints)\n",
    "   - Density features (number of licenses in area)\n",
    "   - Demographic data integration\n",
    "4. **Model Optimization**: Hyperparameter tuning, feature selection, ensemble methods\n",
    "5. **Deployment**: Create API or web interface for predictions\n",
    "\n",
    "### Model Performance:\n",
    "- Classification Accuracy: {test_score:.2%}\n",
    "- Regression R²: {test_r2:.4f}\n",
    "\n",
    "The pipeline is now ready to be adapted with real noise complaint data for a complete analysis."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
