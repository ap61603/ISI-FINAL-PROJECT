# ================================
# NYC Noise Complaints vs SLA Licenses
# End-to-End Pipeline
# ================================

import pathlib
import time
from datetime import datetime, timedelta

import numpy as np
import pandas as pd
import pyarrow as pa
import pyarrow.parquet as pq
from sklearn.neighbors import BallTree

# -------------------
# Project paths
# -------------------
PROJECT_ROOT = pathlib.Path().resolve()
DATA_DIR = PROJECT_ROOT / "data"
RAW_DIR = DATA_DIR / "raw"
INTERIM_DIR = DATA_DIR / "interim"
PROCESSED_DIR = DATA_DIR / "processed"
REPORTS_DIR = PROJECT_ROOT / "reports"

for d in [DATA_DIR, RAW_DIR, INTERIM_DIR, PROCESSED_DIR, REPORTS_DIR]:
    d.mkdir(parents=True, exist_ok=True)

# -------------------
# File locations (replace with your actual files)
# -------------------
PATH_311 = RAW_DIR / "311_Service_Requests_2010_Present.csv"
PATH_SLA = RAW_DIR / "SLA_Active_Licenses.csv"

# -------------------
# Parameters
# -------------------
BOROUGHS = {"BROOKLYN", "STATEN ISLAND"}
PAST_DAYS = 365
NOW = datetime.now()
START_DATE = (NOW - timedelta(days=PAST_DAYS)).date()
VENUE_RADIUS_M = 150
CHUNK_SIZE = 200_000

CLEAN_311_PARQUET = INTERIM_DIR / "311_noise_last_year.parquet"
CLEAN_SLA_PARQUET = INTERIM_DIR / "sla_active_bk_si.parquet"
MATCHES_PARQUET = PROCESSED_DIR / "noise_matches_venues.parquet"

# -------------------
# Helpers
# -------------------
def parse_datetime_safe(val):
    if pd.isna(val):
        return pd.NaT
    return pd.to_datetime(val, errors="coerce")

def to_radians(lat_series, lon_series):
    return np.radians(np.c_[lat_series.astype(float), lon_series.astype(float)])

def meters_to_radians(meters):
    earth_radius_m = 6_371_008.8
    return meters / earth_radius_m

# -------------------
# Process 311 complaints
# -------------------
def is_noise_complaint(row):
    ct = str(row.get("Complaint Type", "")).upper()
    desc = str(row.get("Descriptor", "")).upper()
    return ("NOISE" in ct) or ("NOISE" in desc)

def process_311_stream(path_csv, out_parquet):
    frames = []
    total_rows, kept_rows = 0, 0
    for chunk in pd.read_csv(path_csv, chunksize=CHUNK_SIZE, low_memory=False):
        total_rows += len(chunk)
        chunk.columns = [c.strip() for c in chunk.columns]

        if "Created Date" in chunk.columns:
            chunk["Created Date"] = chunk["Created Date"].apply(parse_datetime_safe)
            chunk = chunk[chunk["Created Date"].dt.date >= START_DATE]

        if "Borough" in chunk.columns:
            chunk["Borough"] = chunk["Borough"].astype(str).str.upper().str.strip()
            chunk = chunk[chunk["Borough"].isin(BOROUGHS)]

        chunk = chunk[chunk.apply(is_noise_complaint, axis=1)]
        if "Latitude" in chunk.columns and "Longitude" in chunk.columns:
            chunk = chunk.dropna(subset=["Latitude", "Longitude"])

        kept_rows += len(chunk)
        if len(chunk):
            chunk["hour"] = chunk["Created Date"].dt.hour
            chunk["date"] = chunk["Created Date"].dt.date
        frames.append(chunk)

        print(f"Processed {total_rows:,} rows; kept {kept_rows:,} noise rows...")

    df = pd.concat(frames, ignore_index=True) if frames else pd.DataFrame()
    pq.write_table(pa.Table.from_pandas(df), out_parquet)
    print(f"Wrote {len(df):,} rows to {out_parquet}")

process_311_stream(PATH_311, CLEAN_311_PARQUET)

# -------------------
# Process SLA licenses
# -------------------
def normalize_borough_from_sla(row):
    county = str(row.get("County", "")).upper().strip()
    mapping = {"KINGS": "BROOKLYN", "RICHMOND": "STATEN ISLAND"}
    return mapping.get(county, "")

def process_sla(path_csv, out_parquet):
    df = pd.read_csv(path_csv, low_memory=False)
    df.columns = [c.strip() for c in df.columns]
    df["Borough"] = df.apply(normalize_borough_from_sla, axis=1)
    df = df[df["Borough"].isin(BOROUGHS)]
    df = df.dropna(subset=["Latitude", "Longitude"])
    df["Latitude"] = pd.to_numeric(df["Latitude"], errors="coerce")
    df["Longitude"] = pd.to_numeric(df["Longitude"], errors="coerce")
    df = df.dropna(subset=["Latitude", "Longitude"])
    pq.write_table(pa.Table.from_pandas(df), out_parquet)
    print(f"Wrote {len(df):,} venues to {out_parquet}")

process_sla(PATH_SLA, CLEAN_SLA_PARQUET)

# -------------------
# Match complaints to venues
# -------------------
def build_balltree(venues_df):
    coords_rad = to_radians(venues_df["Latitude"], venues_df["Longitude"])
    return BallTree(coords_rad, metric="haversine")

def match_complaints_to_venues(complaints_df, venues_df, radius_m=150):
    if len(venues_df) == 0 or len(complaints_df) == 0:
        return complaints_df.assign(near_venue=False)
    tree = build_balltree(venues_df)
    comp_rad = to_radians(complaints_df["Latitude"], complaints_df["Longitude"])
    dist, ind = tree.query(comp_rad, k=1)
    dist_m = dist.flatten() * 6_371_008.8
    complaints_df["near_venue"] = dist_m <= radius_m
    return complaints_df

df_311 = pq.read_table(CLEAN_311_PARQUET).to_pandas()
df_sla = pq.read_table(CLEAN_SLA_PARQUET).to_pandas()

matched_frames = []
for b in BOROUGHS:
    c_b = df_311[df_311["Borough"] == b].reset_index(drop=True)
    v_b = df_sla[df_sla["Borough"] == b].reset_index(drop=True)
    print(f"Matching {len(c_b):,} complaints in {b} to {len(v_b):,} venues...")
    matched_b = match_complaints_to_venues(c_b, v_b, VENUE_RADIUS_M)
    matched_frames.append(matched_b)

matched = pd.concat(matched_frames, ignore_index=True)
pq.write_table(pa.Table.from_pandas(matched), MATCHES_PARQUET)
print(f"Wrote matched dataset: {MATCHES_PARQUET} ({len(matched):,} rows)")

# -------------------
# Aggregations
# -------------------
agg_hour = (
    matched.groupby(["Borough", "hour"])
    .agg(complaints=("Complaint Type", "count"),
         near_venue=("near_venue", "sum"))
    .reset_index()
)
agg_hour["pct_near_venue"] = agg_hour["near_venue"] / agg_hour["complaints"]

agg_boro = (
    matched.groupby(["Borough"])
    .agg(complaints=("Complaint Type", "count"),
         near_venue=("near_venue", "sum"))
    .reset_index()
)
agg_boro["pct_near_venue"] = agg_boro["near_venue"] / agg_boro["complaints"]

print("\nHourly sample:\n", agg_hour.head())
print("\nBorough totals:\n", agg_boro)
